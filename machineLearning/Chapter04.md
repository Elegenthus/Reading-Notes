### Chapter04 决策树
---
* 根节点包含样本全集，从根节点到每个叶节点的路径对应了一个判定测试序列
* 目的：产生一棵泛化能力强，即处理未见示例能力强的决策树
    * 先验分布：与实验结果无关，在进行统计之前根据其他有关参数的知识而得到的分布
    * 后验分布：抽样以后得到的结果（条件概率）
* 划分选择
    * 希望决策树的分支节点包含的样本尽可能属于同一类别，即结点的“纯度”越来越高
    * 信息增益
        * 信息熵
            * 度量样本集合纯度最常用的一种指标 -> 整体集合
            * 计算公式
        * 信息增益
            * 关于每一个属性
            * 信息增益越大，意味着使用属性a来进行划分所获得的“纯度提升”越大
            * ID3决策树学习算法属性划分依据
                * 信息增益最大的被选为划分属性
            * 对于可取值数目较多的属性有所偏好
    * 增益率
        * 对于可取值数目较少的属性有所偏好
        * C4.5决策树算法先从候选划分属性中找出信息增益高出平均水平的属性，再从其中选择增益率最高的。
    * 基尼指数
        * CART决策树算法划分属性依据
        * 反应从数据集中随机选两个样本，其类别标记不一样的概率
        * 值越小，数据集的纯度越高
        * 选择划分后使得基尼指数最小的属性作为最优划分属性
* 剪枝处理
    * 决策树学习算法中对付“过拟合”的主要手段
    * 通过剪掉一些分支来降低过拟合的风险
    * 基本策略
        * 预剪枝
            * 在决策树生成过程中，对每个结点在划分前进行估计，若当前结点划分后不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点
            * 只有一层划分的决策树，称为“决策树桩”
            * 使得很多分支没有展开，降低过拟合的风险，显著减少了训练时间开销和测试开销
            * 带来了欠拟合的风险
        * 后剪枝
            * 先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该结点对应的子树替换成叶节点能够提升决策树的泛化性能，则将该子树替换为叶节点
            * 欠拟合风险小
            * 训练时间开销很大
    * 使用留出法，留出一部分作为测试集来，算根据决策树得出判断的正确率，和划分前作比较，以判断整棵树的泛化性能是否提升
* 连续与缺失值
    * 连续值处理
        * 二分法 -> C4.5决策树算法中使用的策略
            * 确定一个中心点作为划分点，大于其的算一类，小于的算一类，计算信息增益，选出使得信息增益最大的划分点
        * 与离散值不同，若当前节点的划分属性为连续属性，该属性还可以作为其后代结点的划分属性
    * 缺失值处理
        * 使用推广的信息增益计算式
            * 仅计算无缺失样本情况，并再最终结果上乘以 无缺失样本所占的比例 
* 多变量决策树
    * 不是为每个非叶节点找一个最优划分属性，而是试图建立一个合适的线性分类器，即将属性作为参数代入表达式进行计算
* 增量学习
    * 收到新样本后对学得的模型进行调整
    * 增量学习可以有效地降低每次接收到新样本后的训练时间开销
    * 多步增量学习后得到的模型会与基于全部数据训练而得到的模型有较大差别


